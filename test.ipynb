{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d5d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/jqiao/anaconda3/envs/modernvbert/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/scratch/jqiao/anaconda3/envs/modernvbert/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 19:45:40 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 12-12 19:45:40 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 12-12 19:45:41 [interface.py:201] Failed to import from vllm._C: ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n",
      "INFO 12-12 19:45:44 [utils.py:253] non-default args: {'enable_prefix_caching': False, 'disable_log_stats': True, 'mm_processor_cache_gb': 0, 'logits_processors': [<class 'vllm.model_executor.models.deepseek_ocr.NGramPerReqLogitsProcessor'>], 'model': 'deepseek-ai/DeepSeek-OCR'}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Device string must not be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create model instance\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeepseek-ai/DeepSeek-OCR\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_prefix_caching\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_processor_cache_gb\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processors\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mNGramPerReqLogitsProcessor\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/scratch/jqiao/anaconda3/envs/modernvbert/lib/python3.12/site-packages/vllm/entrypoints/llm.py:343\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    348\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/scratch/jqiao/anaconda3/envs/modernvbert/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:166\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m vllm_config = \u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m executor_class = Executor.get_class(vllm_config)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m envs.VLLM_ENABLE_V1_MULTIPROCESSING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/scratch/jqiao/anaconda3/envs/modernvbert/lib/python3.12/site-packages/vllm/engine/arg_utils.py:1342\u001b[39m, in \u001b[36mEngineArgs.create_engine_config\u001b[39m\u001b[34m(self, usage_context, headless)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1336\u001b[39m \u001b[33;03mCreate the VllmConfig.\u001b[39;00m\n\u001b[32m   1337\u001b[39m \n\u001b[32m   1338\u001b[39m \u001b[33;03mNOTE: If VllmConfig is incompatible, we raise an error.\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1340\u001b[39m current_platform.pre_register_and_update()\n\u001b[32m-> \u001b[39m\u001b[32m1342\u001b[39m device_config = \u001b[43mDeviceConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_platform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[38;5;66;03m# Check if the model is a speculator and override model/tokenizer/config\u001b[39;00m\n\u001b[32m   1345\u001b[39m \u001b[38;5;66;03m# BEFORE creating ModelConfig, so the config is created with the target model\u001b[39;00m\n\u001b[32m   1346\u001b[39m \u001b[38;5;66;03m# Skip speculator detection for cloud storage models (eg: S3, GCS) since\u001b[39;00m\n\u001b[32m   1347\u001b[39m \u001b[38;5;66;03m# HuggingFace cannot load configs directly from S3 URLs. S3 models can still\u001b[39;00m\n\u001b[32m   1348\u001b[39m \u001b[38;5;66;03m# use speculators with explicit --speculative-config.\u001b[39;00m\n\u001b[32m   1349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cloud_storage(\u001b[38;5;28mself\u001b[39m.model):\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/scratch/jqiao/anaconda3/envs/modernvbert/lib/python3.12/site-packages/vllm/config/device.py:75\u001b[39m, in \u001b[36mDeviceConfig.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# Set device with device type\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28mself\u001b[39m.device = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Device string must not be empty"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# Create model instance\n",
    "llm = LLM(\n",
    "    model=\"deepseek-ai/DeepSeek-OCR\",\n",
    "    enable_prefix_caching=False,\n",
    "    mm_processor_cache_gb=0,\n",
    "    logits_processors=[NGramPerReqLogitsProcessor]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7406c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batched input with your image file\n",
    "image_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\n",
    "image_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\n",
    "prompt = \"<image>\\nFree OCR.\"\n",
    "\n",
    "model_input = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": image_1}\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"multi_modal_data\": {\"image\": image_2}\n",
    "    }\n",
    "]\n",
    "\n",
    "sampling_param = SamplingParams(\n",
    "            temperature=0.0,\n",
    "            max_tokens=8192,\n",
    "            # ngram logit processor args\n",
    "            extra_args=dict(\n",
    "                ngram_size=30,\n",
    "                window_size=90,\n",
    "                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n",
    "            ),\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "# Generate output\n",
    "model_outputs = llm.generate(model_input, sampling_param)\n",
    "\n",
    "# Print output\n",
    "for output in model_outputs:\n",
    "    print(output.outputs[0].text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modernvbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

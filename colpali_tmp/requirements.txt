conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia


https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.1.post1/flash_attn-2.7.1.post1+cu12torch2.4cxx11abiFALSE-cp311-cp311-linux_x86_64.whl



import flash_attn_2_cuda as flash_attn_gpu

pip install pynvml
pip install wandb
pip install transformers==4.53.1